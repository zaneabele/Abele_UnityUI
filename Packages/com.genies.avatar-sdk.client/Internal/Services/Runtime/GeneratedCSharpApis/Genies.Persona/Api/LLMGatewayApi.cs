#pragma warning disable CS0472
/* 
 * FastAPI
 *
 * No description provided (generated by Swagger Codegen https://github.com/swagger-api/swagger-codegen)
 *
 * OpenAPI spec version: 0.4.7
 * 
 * Generated by: https://github.com/swagger-api/swagger-codegen.git
 */
using System;
using System.Collections.Generic;
using System.Collections.ObjectModel;
using System.Linq;
using RestSharp;
using Genies.Persona.Client;
using Genies.Persona.Model;

namespace Genies.Persona.Api
{
    /// <summary>
    /// Represents a collection of functions to interact with the API endpoints
    /// </summary>
        public interface ILLMGatewayApi : IApiAccessor
    {
        #region Synchronous Operations
        /// <summary>
        /// Execute LLM inference using a prompt template from PromptLayer
        /// </summary>
        /// <remarks>
        /// Execute LLM inference with prompt name, inputs, and optional images
        /// </remarks>
        /// <exception cref="Genies.Persona.Client.ApiException">Thrown when fails to make API call</exception>
        /// <param name="body"></param>
        /// <returns>Object</returns>
        Object InferLlmV1LlmInferPost (LLMInferenceRequest body);

        /// <summary>
        /// Execute LLM inference using a prompt template from PromptLayer
        /// </summary>
        /// <remarks>
        /// Execute LLM inference with prompt name, inputs, and optional images
        /// </remarks>
        /// <exception cref="Genies.Persona.Client.ApiException">Thrown when fails to make API call</exception>
        /// <param name="body"></param>
        /// <returns>ApiResponse of Object</returns>
        ApiResponse<Object> InferLlmV1LlmInferPostWithHttpInfo (LLMInferenceRequest body);
        /// <summary>
        /// List available LLM models
        /// </summary>
        /// <remarks>
        /// Returns static list of supported LLM models (OpenAI and Gemini)
        /// </remarks>
        /// <exception cref="Genies.Persona.Client.ApiException">Thrown when fails to make API call</exception>
        /// <returns>InlineResponse2001</returns>
        InlineResponse2001 ListModelsV1LlmModelsGet ();

        /// <summary>
        /// List available LLM models
        /// </summary>
        /// <remarks>
        /// Returns static list of supported LLM models (OpenAI and Gemini)
        /// </remarks>
        /// <exception cref="Genies.Persona.Client.ApiException">Thrown when fails to make API call</exception>
        /// <returns>ApiResponse of InlineResponse2001</returns>
        ApiResponse<InlineResponse2001> ListModelsV1LlmModelsGetWithHttpInfo ();
        /// <summary>
        /// List available prompts from PromptLayer
        /// </summary>
        /// <remarks>
        /// Returns list of available prompt templates
        /// </remarks>
        /// <exception cref="Genies.Persona.Client.ApiException">Thrown when fails to make API call</exception>
        /// <returns>InlineResponse200</returns>
        InlineResponse200 ListPromptsV1LlmPromptsGet ();

        /// <summary>
        /// List available prompts from PromptLayer
        /// </summary>
        /// <remarks>
        /// Returns list of available prompt templates
        /// </remarks>
        /// <exception cref="Genies.Persona.Client.ApiException">Thrown when fails to make API call</exception>
        /// <returns>ApiResponse of InlineResponse200</returns>
        ApiResponse<InlineResponse200> ListPromptsV1LlmPromptsGetWithHttpInfo ();
        #endregion Synchronous Operations
        #region Asynchronous Operations
        /// <summary>
        /// Execute LLM inference using a prompt template from PromptLayer
        /// </summary>
        /// <remarks>
        /// Execute LLM inference with prompt name, inputs, and optional images
        /// </remarks>
        /// <exception cref="Genies.Persona.Client.ApiException">Thrown when fails to make API call</exception>
        /// <param name="body"></param>
        /// <returns>Task of Object</returns>
        System.Threading.Tasks.Task<Object> InferLlmV1LlmInferPostAsync (LLMInferenceRequest body);

        /// <summary>
        /// Execute LLM inference using a prompt template from PromptLayer
        /// </summary>
        /// <remarks>
        /// Execute LLM inference with prompt name, inputs, and optional images
        /// </remarks>
        /// <exception cref="Genies.Persona.Client.ApiException">Thrown when fails to make API call</exception>
        /// <param name="body"></param>
        /// <returns>Task of ApiResponse (Object)</returns>
        System.Threading.Tasks.Task<ApiResponse<Object>> InferLlmV1LlmInferPostAsyncWithHttpInfo (LLMInferenceRequest body);
        /// <summary>
        /// List available LLM models
        /// </summary>
        /// <remarks>
        /// Returns static list of supported LLM models (OpenAI and Gemini)
        /// </remarks>
        /// <exception cref="Genies.Persona.Client.ApiException">Thrown when fails to make API call</exception>
        /// <returns>Task of InlineResponse2001</returns>
        System.Threading.Tasks.Task<InlineResponse2001> ListModelsV1LlmModelsGetAsync ();

        /// <summary>
        /// List available LLM models
        /// </summary>
        /// <remarks>
        /// Returns static list of supported LLM models (OpenAI and Gemini)
        /// </remarks>
        /// <exception cref="Genies.Persona.Client.ApiException">Thrown when fails to make API call</exception>
        /// <returns>Task of ApiResponse (InlineResponse2001)</returns>
        System.Threading.Tasks.Task<ApiResponse<InlineResponse2001>> ListModelsV1LlmModelsGetAsyncWithHttpInfo ();
        /// <summary>
        /// List available prompts from PromptLayer
        /// </summary>
        /// <remarks>
        /// Returns list of available prompt templates
        /// </remarks>
        /// <exception cref="Genies.Persona.Client.ApiException">Thrown when fails to make API call</exception>
        /// <returns>Task of InlineResponse200</returns>
        System.Threading.Tasks.Task<InlineResponse200> ListPromptsV1LlmPromptsGetAsync ();

        /// <summary>
        /// List available prompts from PromptLayer
        /// </summary>
        /// <remarks>
        /// Returns list of available prompt templates
        /// </remarks>
        /// <exception cref="Genies.Persona.Client.ApiException">Thrown when fails to make API call</exception>
        /// <returns>Task of ApiResponse (InlineResponse200)</returns>
        System.Threading.Tasks.Task<ApiResponse<InlineResponse200>> ListPromptsV1LlmPromptsGetAsyncWithHttpInfo ();
        #endregion Asynchronous Operations
    }

    /// <summary>
    /// Represents a collection of functions to interact with the API endpoints
    /// </summary>
        public partial class LLMGatewayApi : ILLMGatewayApi
    {
        private Genies.Persona.Client.ExceptionFactory _exceptionFactory = (name, response) => null;

        /// <summary>
        /// Initializes a new instance of the <see cref="LLMGatewayApi"/> class.
        /// </summary>
        /// <returns></returns>
        public LLMGatewayApi(String basePath)
        {
            this.Configuration = new Genies.Persona.Client.Configuration { BasePath = basePath };

            ExceptionFactory = Genies.Persona.Client.Configuration.DefaultExceptionFactory;
        }

        /// <summary>
        /// Initializes a new instance of the <see cref="LLMGatewayApi"/> class
        /// </summary>
        /// <returns></returns>
        public LLMGatewayApi()
        {
            this.Configuration = Genies.Persona.Client.Configuration.Default;

            ExceptionFactory = Genies.Persona.Client.Configuration.DefaultExceptionFactory;
        }

        /// <summary>
        /// Initializes a new instance of the <see cref="LLMGatewayApi"/> class
        /// using Configuration object
        /// </summary>
        /// <param name="configuration">An instance of Configuration</param>
        /// <returns></returns>
        public LLMGatewayApi(Genies.Persona.Client.Configuration configuration = null)
        {
            if (configuration == null) // use the default one in Configuration
                this.Configuration = Genies.Persona.Client.Configuration.Default;
            else
                this.Configuration = configuration;

            ExceptionFactory = Genies.Persona.Client.Configuration.DefaultExceptionFactory;
        }

        /// <summary>
        /// Gets the base path of the API client.
        /// </summary>
        /// <value>The base path</value>
        public String GetBasePath()
        {
            return this.Configuration.ApiClient.RestClient.BaseUrl.ToString();
        }

        /// <summary>
        /// Sets the base path of the API client.
        /// </summary>
        /// <value>The base path</value>
        [Obsolete("SetBasePath is deprecated, please do 'Configuration.ApiClient = new ApiClient(\"http://new-path\")' instead.")]
        public void SetBasePath(String basePath)
        {
            // do nothing
        }

        /// <summary>
        /// Gets or sets the configuration object
        /// </summary>
        /// <value>An instance of the Configuration</value>
        public Genies.Persona.Client.Configuration Configuration {get; set;}

        /// <summary>
        /// Provides a factory method hook for the creation of exceptions.
        /// </summary>
        public Genies.Persona.Client.ExceptionFactory ExceptionFactory
        {
            get
            {
                if (_exceptionFactory != null && _exceptionFactory.GetInvocationList().Length > 1)
                {
                    throw new InvalidOperationException("Multicast delegate for ExceptionFactory is unsupported.");
                }
                return _exceptionFactory;
            }
            set { _exceptionFactory = value; }
        }

        /// <summary>
        /// Gets the default header.
        /// </summary>
        /// <returns>Dictionary of HTTP header</returns>
        [Obsolete("DefaultHeader is deprecated, please use Configuration.DefaultHeader instead.")]
        public IDictionary<String, String> DefaultHeader()
        {
            return new ReadOnlyDictionary<string, string>(this.Configuration.DefaultHeader);
        }

        /// <summary>
        /// Add default header.
        /// </summary>
        /// <param name="key">Header field name.</param>
        /// <param name="value">Header field value.</param>
        /// <returns></returns>
        [Obsolete("AddDefaultHeader is deprecated, please use Configuration.AddDefaultHeader instead.")]
        public void AddDefaultHeader(string key, string value)
        {
            this.Configuration.AddDefaultHeader(key, value);
        }

        /// <summary>
        /// Execute LLM inference using a prompt template from PromptLayer Execute LLM inference with prompt name, inputs, and optional images
        /// </summary>
        /// <exception cref="Genies.Persona.Client.ApiException">Thrown when fails to make API call</exception>
        /// <param name="body"></param>
        /// <returns>Object</returns>
        public Object InferLlmV1LlmInferPost (LLMInferenceRequest body)
        {
             ApiResponse<Object> localVarResponse = InferLlmV1LlmInferPostWithHttpInfo(body);
             return localVarResponse.Data;
        }

        /// <summary>
        /// Execute LLM inference using a prompt template from PromptLayer Execute LLM inference with prompt name, inputs, and optional images
        /// </summary>
        /// <exception cref="Genies.Persona.Client.ApiException">Thrown when fails to make API call</exception>
        /// <param name="body"></param>
        /// <returns>ApiResponse of Object</returns>
        public ApiResponse< Object > InferLlmV1LlmInferPostWithHttpInfo (LLMInferenceRequest body)
        {
            // verify the required parameter 'body' is set
            if (body == null)
                throw new ApiException(400, "Missing required parameter 'body' when calling LLMGatewayApi->InferLlmV1LlmInferPost");

            var localVarPath = "/v1/llm/infer";
            var localVarPathParams = new Dictionary<String, String>();
            var localVarQueryParams = new List<KeyValuePair<String, String>>();
            var localVarHeaderParams = new Dictionary<String, String>(this.Configuration.DefaultHeader);
            var localVarFormParams = new Dictionary<String, String>();
            var localVarFileParams = new Dictionary<String, FileParameter>();
            Object localVarPostBody = null;

            // to determine the Content-Type header
            String[] localVarHttpContentTypes = new String[] {
                "application/json"
            };
            String localVarHttpContentType = this.Configuration.ApiClient.SelectHeaderContentType(localVarHttpContentTypes);

            // to determine the Accept header
            String[] localVarHttpHeaderAccepts = new String[] {
                "application/json"
            };
            String localVarHttpHeaderAccept = this.Configuration.ApiClient.SelectHeaderAccept(localVarHttpHeaderAccepts);
            if (localVarHttpHeaderAccept != null)
                localVarHeaderParams.Add("Accept", localVarHttpHeaderAccept);

            if (body != null && body.GetType() != typeof(byte[]))
            {
                localVarPostBody = this.Configuration.ApiClient.Serialize(body); // http body (model) parameter
            }
            else
            {
                localVarPostBody = body; // byte array
            }
            // authentication (bearerAuth) required
            // bearer required
            if (!String.IsNullOrEmpty(this.Configuration.AccessToken))
            {
                localVarHeaderParams["Authorization"] = "Bearer " + this.Configuration.AccessToken;
            }

            // make the HTTP request
            IRestResponse localVarResponse = (IRestResponse) this.Configuration.ApiClient.CallApi(localVarPath,
                Method.POST, localVarQueryParams, localVarPostBody, localVarHeaderParams, localVarFormParams, localVarFileParams,
                localVarPathParams, localVarHttpContentType);

            int localVarStatusCode = (int) localVarResponse.StatusCode;

            if (ExceptionFactory != null)
            {
                Exception exception = ExceptionFactory("InferLlmV1LlmInferPost", localVarResponse);
                if (exception != null) throw exception;
            }

            return new ApiResponse<Object>(localVarStatusCode,
                localVarResponse.Headers.ToDictionary(x => x.Name, x => string.Join(",", x.Value)),
                (Object) this.Configuration.ApiClient.Deserialize(localVarResponse, typeof(Object)));
        }

        /// <summary>
        /// Execute LLM inference using a prompt template from PromptLayer Execute LLM inference with prompt name, inputs, and optional images
        /// </summary>
        /// <exception cref="Genies.Persona.Client.ApiException">Thrown when fails to make API call</exception>
        /// <param name="body"></param>
        /// <returns>Task of Object</returns>
        public async System.Threading.Tasks.Task<Object> InferLlmV1LlmInferPostAsync (LLMInferenceRequest body)
        {
             ApiResponse<Object> localVarResponse = await InferLlmV1LlmInferPostAsyncWithHttpInfo(body);
             return localVarResponse.Data;

        }

        /// <summary>
        /// Execute LLM inference using a prompt template from PromptLayer Execute LLM inference with prompt name, inputs, and optional images
        /// </summary>
        /// <exception cref="Genies.Persona.Client.ApiException">Thrown when fails to make API call</exception>
        /// <param name="body"></param>
        /// <returns>Task of ApiResponse (Object)</returns>
        public async System.Threading.Tasks.Task<ApiResponse<Object>> InferLlmV1LlmInferPostAsyncWithHttpInfo (LLMInferenceRequest body)
        {
            // verify the required parameter 'body' is set
            if (body == null)
                throw new ApiException(400, "Missing required parameter 'body' when calling LLMGatewayApi->InferLlmV1LlmInferPost");

            var localVarPath = "/v1/llm/infer";
            var localVarPathParams = new Dictionary<String, String>();
            var localVarQueryParams = new List<KeyValuePair<String, String>>();
            var localVarHeaderParams = new Dictionary<String, String>(this.Configuration.DefaultHeader);
            var localVarFormParams = new Dictionary<String, String>();
            var localVarFileParams = new Dictionary<String, FileParameter>();
            Object localVarPostBody = null;

            // to determine the Content-Type header
            String[] localVarHttpContentTypes = new String[] {
                "application/json"
            };
            String localVarHttpContentType = this.Configuration.ApiClient.SelectHeaderContentType(localVarHttpContentTypes);

            // to determine the Accept header
            String[] localVarHttpHeaderAccepts = new String[] {
                "application/json"
            };
            String localVarHttpHeaderAccept = this.Configuration.ApiClient.SelectHeaderAccept(localVarHttpHeaderAccepts);
            if (localVarHttpHeaderAccept != null)
                localVarHeaderParams.Add("Accept", localVarHttpHeaderAccept);

            if (body != null && body.GetType() != typeof(byte[]))
            {
                localVarPostBody = this.Configuration.ApiClient.Serialize(body); // http body (model) parameter
            }
            else
            {
                localVarPostBody = body; // byte array
            }
            // authentication (bearerAuth) required
            // bearer required
            if (!String.IsNullOrEmpty(this.Configuration.AccessToken))
            {
                localVarHeaderParams["Authorization"] = "Bearer " + this.Configuration.AccessToken;
            }

            // make the HTTP request
            IRestResponse localVarResponse = (IRestResponse) await this.Configuration.ApiClient.CallApiAsync(localVarPath,
                Method.POST, localVarQueryParams, localVarPostBody, localVarHeaderParams, localVarFormParams, localVarFileParams,
                localVarPathParams, localVarHttpContentType);

            int localVarStatusCode = (int) localVarResponse.StatusCode;

            if (ExceptionFactory != null)
            {
                Exception exception = ExceptionFactory("InferLlmV1LlmInferPost", localVarResponse);
                if (exception != null) throw exception;
            }

            return new ApiResponse<Object>(localVarStatusCode,
                localVarResponse.Headers.ToDictionary(x => x.Name, x => string.Join(",", x.Value)),
                (Object) this.Configuration.ApiClient.Deserialize(localVarResponse, typeof(Object)));
        }

        /// <summary>
        /// List available LLM models Returns static list of supported LLM models (OpenAI and Gemini)
        /// </summary>
        /// <exception cref="Genies.Persona.Client.ApiException">Thrown when fails to make API call</exception>
        /// <returns>InlineResponse2001</returns>
        public InlineResponse2001 ListModelsV1LlmModelsGet ()
        {
             ApiResponse<InlineResponse2001> localVarResponse = ListModelsV1LlmModelsGetWithHttpInfo();
             return localVarResponse.Data;
        }

        /// <summary>
        /// List available LLM models Returns static list of supported LLM models (OpenAI and Gemini)
        /// </summary>
        /// <exception cref="Genies.Persona.Client.ApiException">Thrown when fails to make API call</exception>
        /// <returns>ApiResponse of InlineResponse2001</returns>
        public ApiResponse< InlineResponse2001 > ListModelsV1LlmModelsGetWithHttpInfo ()
        {

            var localVarPath = "/v1/llm/models";
            var localVarPathParams = new Dictionary<String, String>();
            var localVarQueryParams = new List<KeyValuePair<String, String>>();
            var localVarHeaderParams = new Dictionary<String, String>(this.Configuration.DefaultHeader);
            var localVarFormParams = new Dictionary<String, String>();
            var localVarFileParams = new Dictionary<String, FileParameter>();
            Object localVarPostBody = null;

            // to determine the Content-Type header
            String[] localVarHttpContentTypes = new String[] {
            };
            String localVarHttpContentType = this.Configuration.ApiClient.SelectHeaderContentType(localVarHttpContentTypes);

            // to determine the Accept header
            String[] localVarHttpHeaderAccepts = new String[] {
                "application/json"
            };
            String localVarHttpHeaderAccept = this.Configuration.ApiClient.SelectHeaderAccept(localVarHttpHeaderAccepts);
            if (localVarHttpHeaderAccept != null)
                localVarHeaderParams.Add("Accept", localVarHttpHeaderAccept);

            // authentication (bearerAuth) required
            // bearer required
            if (!String.IsNullOrEmpty(this.Configuration.AccessToken))
            {
                localVarHeaderParams["Authorization"] = "Bearer " + this.Configuration.AccessToken;
            }

            // make the HTTP request
            IRestResponse localVarResponse = (IRestResponse) this.Configuration.ApiClient.CallApi(localVarPath,
                Method.GET, localVarQueryParams, localVarPostBody, localVarHeaderParams, localVarFormParams, localVarFileParams,
                localVarPathParams, localVarHttpContentType);

            int localVarStatusCode = (int) localVarResponse.StatusCode;

            if (ExceptionFactory != null)
            {
                Exception exception = ExceptionFactory("ListModelsV1LlmModelsGet", localVarResponse);
                if (exception != null) throw exception;
            }

            return new ApiResponse<InlineResponse2001>(localVarStatusCode,
                localVarResponse.Headers.ToDictionary(x => x.Name, x => string.Join(",", x.Value)),
                (InlineResponse2001) this.Configuration.ApiClient.Deserialize(localVarResponse, typeof(InlineResponse2001)));
        }

        /// <summary>
        /// List available LLM models Returns static list of supported LLM models (OpenAI and Gemini)
        /// </summary>
        /// <exception cref="Genies.Persona.Client.ApiException">Thrown when fails to make API call</exception>
        /// <returns>Task of InlineResponse2001</returns>
        public async System.Threading.Tasks.Task<InlineResponse2001> ListModelsV1LlmModelsGetAsync ()
        {
             ApiResponse<InlineResponse2001> localVarResponse = await ListModelsV1LlmModelsGetAsyncWithHttpInfo();
             return localVarResponse.Data;

        }

        /// <summary>
        /// List available LLM models Returns static list of supported LLM models (OpenAI and Gemini)
        /// </summary>
        /// <exception cref="Genies.Persona.Client.ApiException">Thrown when fails to make API call</exception>
        /// <returns>Task of ApiResponse (InlineResponse2001)</returns>
        public async System.Threading.Tasks.Task<ApiResponse<InlineResponse2001>> ListModelsV1LlmModelsGetAsyncWithHttpInfo ()
        {

            var localVarPath = "/v1/llm/models";
            var localVarPathParams = new Dictionary<String, String>();
            var localVarQueryParams = new List<KeyValuePair<String, String>>();
            var localVarHeaderParams = new Dictionary<String, String>(this.Configuration.DefaultHeader);
            var localVarFormParams = new Dictionary<String, String>();
            var localVarFileParams = new Dictionary<String, FileParameter>();
            Object localVarPostBody = null;

            // to determine the Content-Type header
            String[] localVarHttpContentTypes = new String[] {
            };
            String localVarHttpContentType = this.Configuration.ApiClient.SelectHeaderContentType(localVarHttpContentTypes);

            // to determine the Accept header
            String[] localVarHttpHeaderAccepts = new String[] {
                "application/json"
            };
            String localVarHttpHeaderAccept = this.Configuration.ApiClient.SelectHeaderAccept(localVarHttpHeaderAccepts);
            if (localVarHttpHeaderAccept != null)
                localVarHeaderParams.Add("Accept", localVarHttpHeaderAccept);

            // authentication (bearerAuth) required
            // bearer required
            if (!String.IsNullOrEmpty(this.Configuration.AccessToken))
            {
                localVarHeaderParams["Authorization"] = "Bearer " + this.Configuration.AccessToken;
            }

            // make the HTTP request
            IRestResponse localVarResponse = (IRestResponse) await this.Configuration.ApiClient.CallApiAsync(localVarPath,
                Method.GET, localVarQueryParams, localVarPostBody, localVarHeaderParams, localVarFormParams, localVarFileParams,
                localVarPathParams, localVarHttpContentType);

            int localVarStatusCode = (int) localVarResponse.StatusCode;

            if (ExceptionFactory != null)
            {
                Exception exception = ExceptionFactory("ListModelsV1LlmModelsGet", localVarResponse);
                if (exception != null) throw exception;
            }

            return new ApiResponse<InlineResponse2001>(localVarStatusCode,
                localVarResponse.Headers.ToDictionary(x => x.Name, x => string.Join(",", x.Value)),
                (InlineResponse2001) this.Configuration.ApiClient.Deserialize(localVarResponse, typeof(InlineResponse2001)));
        }

        /// <summary>
        /// List available prompts from PromptLayer Returns list of available prompt templates
        /// </summary>
        /// <exception cref="Genies.Persona.Client.ApiException">Thrown when fails to make API call</exception>
        /// <returns>InlineResponse200</returns>
        public InlineResponse200 ListPromptsV1LlmPromptsGet ()
        {
             ApiResponse<InlineResponse200> localVarResponse = ListPromptsV1LlmPromptsGetWithHttpInfo();
             return localVarResponse.Data;
        }

        /// <summary>
        /// List available prompts from PromptLayer Returns list of available prompt templates
        /// </summary>
        /// <exception cref="Genies.Persona.Client.ApiException">Thrown when fails to make API call</exception>
        /// <returns>ApiResponse of InlineResponse200</returns>
        public ApiResponse< InlineResponse200 > ListPromptsV1LlmPromptsGetWithHttpInfo ()
        {

            var localVarPath = "/v1/llm/prompts";
            var localVarPathParams = new Dictionary<String, String>();
            var localVarQueryParams = new List<KeyValuePair<String, String>>();
            var localVarHeaderParams = new Dictionary<String, String>(this.Configuration.DefaultHeader);
            var localVarFormParams = new Dictionary<String, String>();
            var localVarFileParams = new Dictionary<String, FileParameter>();
            Object localVarPostBody = null;

            // to determine the Content-Type header
            String[] localVarHttpContentTypes = new String[] {
            };
            String localVarHttpContentType = this.Configuration.ApiClient.SelectHeaderContentType(localVarHttpContentTypes);

            // to determine the Accept header
            String[] localVarHttpHeaderAccepts = new String[] {
                "application/json"
            };
            String localVarHttpHeaderAccept = this.Configuration.ApiClient.SelectHeaderAccept(localVarHttpHeaderAccepts);
            if (localVarHttpHeaderAccept != null)
                localVarHeaderParams.Add("Accept", localVarHttpHeaderAccept);

            // authentication (bearerAuth) required
            // bearer required
            if (!String.IsNullOrEmpty(this.Configuration.AccessToken))
            {
                localVarHeaderParams["Authorization"] = "Bearer " + this.Configuration.AccessToken;
            }

            // make the HTTP request
            IRestResponse localVarResponse = (IRestResponse) this.Configuration.ApiClient.CallApi(localVarPath,
                Method.GET, localVarQueryParams, localVarPostBody, localVarHeaderParams, localVarFormParams, localVarFileParams,
                localVarPathParams, localVarHttpContentType);

            int localVarStatusCode = (int) localVarResponse.StatusCode;

            if (ExceptionFactory != null)
            {
                Exception exception = ExceptionFactory("ListPromptsV1LlmPromptsGet", localVarResponse);
                if (exception != null) throw exception;
            }

            return new ApiResponse<InlineResponse200>(localVarStatusCode,
                localVarResponse.Headers.ToDictionary(x => x.Name, x => string.Join(",", x.Value)),
                (InlineResponse200) this.Configuration.ApiClient.Deserialize(localVarResponse, typeof(InlineResponse200)));
        }

        /// <summary>
        /// List available prompts from PromptLayer Returns list of available prompt templates
        /// </summary>
        /// <exception cref="Genies.Persona.Client.ApiException">Thrown when fails to make API call</exception>
        /// <returns>Task of InlineResponse200</returns>
        public async System.Threading.Tasks.Task<InlineResponse200> ListPromptsV1LlmPromptsGetAsync ()
        {
             ApiResponse<InlineResponse200> localVarResponse = await ListPromptsV1LlmPromptsGetAsyncWithHttpInfo();
             return localVarResponse.Data;

        }

        /// <summary>
        /// List available prompts from PromptLayer Returns list of available prompt templates
        /// </summary>
        /// <exception cref="Genies.Persona.Client.ApiException">Thrown when fails to make API call</exception>
        /// <returns>Task of ApiResponse (InlineResponse200)</returns>
        public async System.Threading.Tasks.Task<ApiResponse<InlineResponse200>> ListPromptsV1LlmPromptsGetAsyncWithHttpInfo ()
        {

            var localVarPath = "/v1/llm/prompts";
            var localVarPathParams = new Dictionary<String, String>();
            var localVarQueryParams = new List<KeyValuePair<String, String>>();
            var localVarHeaderParams = new Dictionary<String, String>(this.Configuration.DefaultHeader);
            var localVarFormParams = new Dictionary<String, String>();
            var localVarFileParams = new Dictionary<String, FileParameter>();
            Object localVarPostBody = null;

            // to determine the Content-Type header
            String[] localVarHttpContentTypes = new String[] {
            };
            String localVarHttpContentType = this.Configuration.ApiClient.SelectHeaderContentType(localVarHttpContentTypes);

            // to determine the Accept header
            String[] localVarHttpHeaderAccepts = new String[] {
                "application/json"
            };
            String localVarHttpHeaderAccept = this.Configuration.ApiClient.SelectHeaderAccept(localVarHttpHeaderAccepts);
            if (localVarHttpHeaderAccept != null)
                localVarHeaderParams.Add("Accept", localVarHttpHeaderAccept);

            // authentication (bearerAuth) required
            // bearer required
            if (!String.IsNullOrEmpty(this.Configuration.AccessToken))
            {
                localVarHeaderParams["Authorization"] = "Bearer " + this.Configuration.AccessToken;
            }

            // make the HTTP request
            IRestResponse localVarResponse = (IRestResponse) await this.Configuration.ApiClient.CallApiAsync(localVarPath,
                Method.GET, localVarQueryParams, localVarPostBody, localVarHeaderParams, localVarFormParams, localVarFileParams,
                localVarPathParams, localVarHttpContentType);

            int localVarStatusCode = (int) localVarResponse.StatusCode;

            if (ExceptionFactory != null)
            {
                Exception exception = ExceptionFactory("ListPromptsV1LlmPromptsGet", localVarResponse);
                if (exception != null) throw exception;
            }

            return new ApiResponse<InlineResponse200>(localVarStatusCode,
                localVarResponse.Headers.ToDictionary(x => x.Name, x => string.Join(",", x.Value)),
                (InlineResponse200) this.Configuration.ApiClient.Deserialize(localVarResponse, typeof(InlineResponse200)));
        }

    }
}
